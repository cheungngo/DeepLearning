{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_sigurdurskuli.ipynb",
      "provenance": [],
      "mount_file_id": "1X5N4Jth1NqwQxJRJg1S0zXCM4uXZG3uM",
      "authorship_tag": "ABX9TyMTwdfvVWcJQNeOf+rCawzF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cheungngo/DeepLearning/blob/master/Music/01_sigurdurskuli.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to Generate Music using a LSTM Neural Network in Keras"
      ],
      "metadata": {
        "id": "VfgnKZ8ATeKF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://sigurdurskuli.com/posts/music-generation"
      ],
      "metadata": {
        "id": "GmU4OAHJTh1H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the data"
      ],
      "metadata": {
        "id": "Vw3fPDjAKFF4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have examined the data and determined that the features that we want to use are the notes and chords as the input and output of our LSTM network it is time to prepare the data for the network.\n",
        "\n",
        "First, we will load the data into an array as can be seen in the code snippet below:"
      ],
      "metadata": {
        "id": "CP3csogVKIQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"/content/drive/MyDrive/Colab Notebooks/Own Generative AI/Music/classical_midi\" . -r"
      ],
      "metadata": {
        "id": "jJzHS4UZJBxV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from music21 import converter, instrument, note, chord, stream\n",
        "import glob"
      ],
      "metadata": {
        "id": "tvnL2-x2IGPR"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "yBQL_VvAIETz"
      },
      "outputs": [],
      "source": [
        "notes = []\n",
        "\n",
        "for file in glob.glob(\"classical_midi/*.mid\"):\n",
        "    midi = converter.parse(file)\n",
        "    notes_to_parse = None\n",
        "\n",
        "    parts = instrument.partitionByInstrument(midi)\n",
        "\n",
        "    if parts: # file has instrument parts\n",
        "        notes_to_parse = parts.parts[0].recurse()\n",
        "    else: # file has notes in a flat structure\n",
        "        notes_to_parse = midi.flat.notes\n",
        "\n",
        "    for element in notes_to_parse:\n",
        "        if isinstance(element, note.Note):\n",
        "            notes.append(str(element.pitch))\n",
        "        elif isinstance(element, chord.Chord):\n",
        "            notes.append('.'.join(str(n) for n in element.normalOrder))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "notes[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woqUl9dvKLOs",
        "outputId": "ab94b487-9f84-4034-ca50-4a7803198df6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['G#4', 'C5', 'C#5', 'E-5', 'F5', 'C#5', 'E-5', 'F5', 'G5', 'E-5']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start by loading each file into a Music21 stream object using the converter.parse(file) function. *Using that stream object we get a list of all the notes and chords in the file. We append the pitch of every note object using its string notation since the most significant parts of the note can be recreated using the string notation of the pitch. And we append every chord by encoding the id of every note in the chord together into a single string, with each note being separated by a dot. These encodings allows us to easily decode the output generated by the network into the correct notes and chords.\n",
        "\n",
        "Now that we have put all the notes and chords into a sequential list we can create the sequences that will serve as the input of our network."
      ],
      "metadata": {
        "id": "C1DYAmHdKVrp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we will create a mapping function to map from string-based categorical data to integer-based numerical data. This is done because neural network perform much better with integer-based numerical data than string-based categorical data. An example of a categorical to numerical transformation can be seen in Figure 1.\n",
        "\n",
        "Next, we have to create input sequences for the network and their respective outputs. The output for each input sequence will be the first note or chord that comes after the sequence of notes in the input sequence in our list of notes."
      ],
      "metadata": {
        "id": "rjoAYOgqKWLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.utils import np_utils"
      ],
      "metadata": {
        "id": "mMd4GGbFKvHM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_length = 100\n",
        "n_vocab = len(set(notes))\n",
        "\n",
        "# get all pitch names\n",
        "pitchnames = sorted(set(item for item in notes))\n",
        "\n",
        "# create a dictionary to map pitches to integers\n",
        "note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
        "\n",
        "network_input = []\n",
        "network_output = []\n",
        "\n",
        "# create input sequences and the corresponding outputs\n",
        "for i in range(0, len(notes) - sequence_length, 1):\n",
        "    sequence_in = notes[i:i + sequence_length]\n",
        "    sequence_out = notes[i + sequence_length]\n",
        "    network_input.append([note_to_int[char] for char in sequence_in])\n",
        "    network_output.append(note_to_int[sequence_out])\n",
        "\n",
        "n_patterns = len(network_input)\n",
        "\n",
        "# reshape the input into a format compatible with LSTM layers\n",
        "network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
        "# normalize input\n",
        "network_input = network_input / float(n_vocab)\n",
        "\n",
        "network_output = np_utils.to_categorical(network_output)"
      ],
      "metadata": {
        "id": "Rk8SmoiXKZF0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our code example, we have put the length of each sequence to be 100 notes/chords. This means that to predict the next note in the sequence the network has the previous 100 notes to help make the prediction. I highly recommend training the network using different sequence lengths to see the impact different sequence lengths can have on the music generated by the network.\n",
        "\n",
        "The final step in preparing the data for the network is to normalise the input and one-hot encode the output."
      ],
      "metadata": {
        "id": "lCkRbmY9LeaD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "BgdDpQ2aLlvh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally we get to designing the model architecture. In our model we use four different types of layers:\n",
        "\n",
        "LSTM layers is a Recurrent Neural Net layer that takes a sequence as an input and can return either sequences (return_sequences=True) or a matrix.\n",
        "\n",
        "Dropout layers are a regularisation technique that consists of setting a fraction of input units to 0 at each update during the training to prevent overfitting. The fraction is determined by the parameter used with the layer.\n",
        "\n",
        "Dense layers or fully connected layers is a fully connected neural network layer where each input node is connected to each output node.\n",
        "\n",
        "The Activation layer determines what activation function our neural network will use to calculate the output of a node."
      ],
      "metadata": {
        "id": "Vls7mlaZLrCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Activation\n",
        "from keras.layers import BatchNormalization as BatchNorm"
      ],
      "metadata": {
        "id": "MIf15VkzQuza"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(\n",
        "    256,\n",
        "    input_shape=(network_input.shape[1], network_input.shape[2]),\n",
        "    return_sequences=True\n",
        "))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(LSTM(512, return_sequences=True))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dense(256))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(n_vocab))\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
      ],
      "metadata": {
        "id": "apzDelq-LfPO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint"
      ],
      "metadata": {
        "id": "DqmNz6XLRIPc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filepath = \"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"    \n",
        "\n",
        "checkpoint = ModelCheckpoint(\n",
        "    filepath, monitor='loss', \n",
        "    verbose=0,        \n",
        "    save_best_only=True,        \n",
        "    mode='min'\n",
        ")    \n",
        "callbacks_list = [checkpoint]     "
      ],
      "metadata": {
        "id": "w0Og8NzFRFTs"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(network_input, network_output, epochs=200, batch_size=64, callbacks=callbacks_list)"
      ],
      "metadata": {
        "id": "SmkKtbTxSRNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/weights-improvement-116-0.7331-bigger.hdf5 \"/content/drive/MyDrive/Colab Notebooks/Own Generative AI/Music/class_e116.hdf5\""
      ],
      "metadata": {
        "id": "yVfJLH00eLcA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating Music\n"
      ],
      "metadata": {
        "id": "Mgolyo7oXuLv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have finished training the network it is time to have some fun with the network we have spent hours training.\n",
        "\n",
        "To be able to use the neural network to generate music you will have to put it into the same state as before. For simplicity we will reuse code from the training section to prepare the data and set up the network model in the same way as before. Except, that instead of training the network we load the weights that we saved during the training section into the model."
      ],
      "metadata": {
        "id": "wqHNZmk9XxAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"/content/drive/MyDrive/Colab Notebooks/Own Generative AI/Music/class_e116.hdf5\" ."
      ],
      "metadata": {
        "id": "DxD6IA1oegs_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights('class_e116.hdf5')"
      ],
      "metadata": {
        "id": "iq-9Z2JWX11H"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can use the trained model to start generating notes.\n",
        "\n",
        "Since we have a full list of note sequences at our disposal we will pick a random index in the list as our starting point, this allows us to rerun the generation code without changing anything and get different results every time. However, If you wish to control the starting point simply replace the random function with a command line argument.\n",
        "\n",
        "Here we also need to create a mapping function to decode the output of the network. This function will map from numerical data to categorical data (from integers to notes)."
      ],
      "metadata": {
        "id": "eAl6R1KnYBdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = np.random.randint(0, len(network_input)-1)\n",
        "\n",
        "int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
        "\n",
        "pattern = network_input[start]\n",
        "prediction_output = []\n",
        "\n",
        "# generate 500 notes\n",
        "for note_index in range(500):\n",
        "    prediction_input = np.reshape(pattern, (1, len(pattern), 1))\n",
        "    prediction_input = prediction_input / float(n_vocab)\n",
        "\n",
        "    prediction = model.predict(prediction_input, verbose=0)\n",
        "\n",
        "    index = np.argmax(prediction)\n",
        "    result = int_to_note[index]\n",
        "    prediction_output.append(result)\n",
        "\n",
        "    pattern = np.append(pattern, index)\n",
        "    pattern = pattern[1:len(pattern)]"
      ],
      "metadata": {
        "id": "SamsNawXX9Ew"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We chose to generate 500 notes using the network since that is roughly two minutes of music and gives the network plenty of space to create a melody. For each note that we want to generate we have to submit a sequence to the network. The first sequence we submit is the sequence of notes at the starting index. For every subsequent sequence that we use as input, we will remove the first note of the sequence and insert the output of the previous iteration at the end of the sequence.\n",
        "\n"
      ],
      "metadata": {
        "id": "W0JzfrU3YE1D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we collect all the outputs from the network into a single array.\n",
        "\n",
        "Now that we have all the encoded representations of the notes and chords in an array we can start decoding them and creating an array of Note and Chord objects.\n",
        "\n",
        "First we have to determine whether the output we are decoding is a Note or a Chord.\n",
        "\n",
        "If the pattern is a Chord, we have to split the string up into an array of notes. Then we loop through the string representation of each note and create a Note object for each of them. Then we can create a Chord object containing each of these notes.\n",
        "\n",
        "If the pattern is a Note, we create a Note object using the string representation of the pitch contained in the pattern.\n",
        "\n",
        "At the end of each iteration we increase the offset by 0.5 (as we decided in a previous section) and append the Note/Chord object created to a list."
      ],
      "metadata": {
        "id": "S5xfMqFhYQ6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "offset = 0\n",
        "output_notes = []\n",
        "\n",
        "# create note and chord objects based on the values generated by the model\n",
        "\n",
        "for pattern in prediction_output:\n",
        "    # pattern is a chord\n",
        "    if ('.' in pattern) or pattern.isdigit():\n",
        "        notes_in_chord = pattern.split('.')\n",
        "        notes = []\n",
        "        for current_note in notes_in_chord:\n",
        "            new_note = note.Note(int(current_note))\n",
        "            new_note.storedInstrument = instrument.Piano()\n",
        "            notes.append(new_note)\n",
        "        new_chord = chord.Chord(notes)\n",
        "        new_chord.offset = offset\n",
        "        output_notes.append(new_chord)\n",
        "    # pattern is a note\n",
        "    else:\n",
        "        new_note = note.Note(pattern)\n",
        "        new_note.offset = offset\n",
        "        new_note.storedInstrument = instrument.Piano()\n",
        "        output_notes.append(new_note)\n",
        "\n",
        "    # increase offset each iteration so that notes do not stack\n",
        "    offset += 0.5"
      ],
      "metadata": {
        "id": "eskw2pw-YIKa"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_notes[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckqFvVbbe2nh",
        "outputId": "3578fe2f-b1c0-4da1-febe-8733d8828cd5"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<music21.chord.Chord C F>,\n",
              " <music21.chord.Chord C F>,\n",
              " <music21.chord.Chord D F>,\n",
              " <music21.chord.Chord C F>,\n",
              " <music21.chord.Chord D F>,\n",
              " <music21.chord.Chord E- G#>,\n",
              " <music21.chord.Chord C F>,\n",
              " <music21.chord.Chord C F>,\n",
              " <music21.chord.Chord C F>,\n",
              " <music21.chord.Chord B- E->]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "midi_stream = stream.Stream(output_notes)\n",
        "\n",
        "midi_stream.write('midi', fp='test_output.mid')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "Br4_IH4fYYLM",
        "outputId": "bb816678-b3be-4bf3-e00d-1cc84a3597ef"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'test_output.mid'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    }
  ]
}